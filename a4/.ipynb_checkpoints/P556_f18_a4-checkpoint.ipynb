{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab_type": "text",
    "id": "Pklo4QPedGll"
   },
   "source": [
    "# CSCI-P556, Fall 2018\n",
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5mXtO-XLcycH"
   },
   "source": [
    "We will be using the the following data sets in this assignment:\n",
    "\n",
    "\n",
    "\n",
    "*   [Ionosphere](https://archive.ics.uci.edu/ml/datasets/ionosphere)\n",
    "*   [Car Evaluation](https://archive.ics.uci.edu/ml/datasets/car+evaluation)\n",
    "*   [Credit Approval](https://archive.ics.uci.edu/ml/datasets/Credit+Approval)\n",
    "\n",
    "Download them to the same folder as this .ipynb file. Take a look at the 'Data Set Descriptions' links in the above links.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ZnETDTlbpKW"
   },
   "source": [
    "## Problem 1: K-Fold Cross Validation [10 points]\n",
    "\n",
    "Implement k- fold cross validation and select k = 5 to create 5 training and 5 test data sets from each data\n",
    "set and save these 30 files. You will use these data sets for model comparison and parameter selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vCnP8jphbmWN"
   },
   "outputs": [],
   "source": [
    "# Code for problem 1 goes here\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "#Performing one hot encoding for kNN in case of distance calculation for categorical variables\n",
    "def oneHotEncode(data):\n",
    "    no_of_features = data.shape[1]\n",
    "    for col in data.columns.values:\n",
    "        if data[col].dtypes =='object' and no_of_features > 0:\n",
    "            no_of_features -= 1\n",
    "            data = pd.concat([data,pd.get_dummies(data[col], prefix=col)],axis=1)\n",
    "            data.drop([col],axis=1, inplace=True)\n",
    "    return data\n",
    "\n",
    "def k_fold_cross_val(data):\n",
    "    n = int(data.shape[0]/5)\n",
    "    trains_list = []\n",
    "    tests_list = []\n",
    "    trains_list_encoded = []\n",
    "    tests_list_encoded = []\n",
    "    k = 0\n",
    "    #for k in range(data.shape[0]):\n",
    "    while k < data.shape[0] - n + 1:\n",
    "        trains = []\n",
    "        test = data[k:k+n]\n",
    "        if k > 0:\n",
    "            train1 = data[0:k]\n",
    "            trains.append(train1)\n",
    "        if k + n + 1 <= data.shape[0]:\n",
    "            train2 = data[k+n : data.shape[0]]\n",
    "            trains.append(train2)\n",
    "        data_train = pd.concat(trains)\n",
    "        train_encoded = oneHotEncode(data_train.loc[: , data_train.columns != data_train.shape[1] - 1])\n",
    "        test_encoded = oneHotEncode(test.loc[: , test.columns != test.shape[1] - 1])\n",
    "        trains_list_encoded.append((train_encoded , data_train[data_train.shape[1] - 1]))\n",
    "        tests_list_encoded.append((test_encoded , test[test.shape[1] - 1]))\n",
    "        trains_list.append((data_train.loc[: , data_train.columns != data_train.shape[1] - 1] , data_train[data_train.shape[1] - 1]))\n",
    "        tests_list.append((test.loc[: , test.columns != test.shape[1] - 1] , test[test.shape[1] - 1]))\n",
    "        k += n\n",
    "    \n",
    "    return trains_list, tests_list, trains_list_encoded, tests_list_encoded\n",
    "        \n",
    "    \n",
    "    \n",
    "import pandas as pd\n",
    "data_ionosphere = pd.read_table(\"ionosphere.data.txt\", header = None, delimiter = \",\")\n",
    "#data_ionosphere_X = data_ionosphere.loc[:, data_ionosphere.columns != 34]\n",
    "#data_ionosphere_y = data_ionosphere[34]\n",
    "\n",
    "trains_ionosphere, tests_ionosphere, trains_ionosphere_encoded, tests_ionosphere_encoded = k_fold_cross_val(data_ionosphere)\n",
    "\n",
    "\n",
    "\n",
    "data_cars = pd.read_table(\"car.data.txt\", header = None, delimiter = \",\")\n",
    "#data_cars_X = data_cars.loc[:, data_cars.columns != 6]\n",
    "#data_cars_y = data_cars[6]\n",
    "trains_cars, tests_cars, trains_cars_encoded, tests_cars_encoded = k_fold_cross_val(data_cars)\n",
    "\n",
    "\n",
    "data_credit = pd.read_table(\"crx.data.txt\", header = None, delimiter = \",\")\n",
    "#data_credit_X = data_credit.loc[:, data_credit.columns != 15]\n",
    "#data_credit_y = data_credit[15]\n",
    "trains_credit, tests_credit, trains_credit_encoded, tests_credit_encoded = k_fold_cross_val(data_credit)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QEl0kqIobZ6y"
   },
   "source": [
    "## Problem 2: K-Nearest Neighbors (KNN) [30 points]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "34Ak_zYIdwBS"
   },
   "source": [
    "2.1 Implement KNN algorithm with two different distance functions. You can either use an existing distance\n",
    "functions, i.e., Euclidean or design your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZS6BEoOldqiC"
   },
   "outputs": [],
   "source": [
    "# Code for problem 2.1 goes here\n",
    "from operator import itemgetter\n",
    "import math\n",
    "\n",
    "def getEuclideanDist(x, y):\n",
    "    dist = 0\n",
    "    for i in range(len(x)):\n",
    "        dist += pow(x[i] - y[i], 2)\n",
    "    #print(dist)\n",
    "    dist = math.sqrt(dist)\n",
    "    return dist\n",
    "\n",
    "def getKneighbors(train, test, k):\n",
    "    no_of_predictions = 0\n",
    "    test_row_index = 0\n",
    "    for index, x in test[0].iterrows():\n",
    "        distances = []\n",
    "        maxVotes = -1\n",
    "        maxClass = \"\"\n",
    "        votes = {}\n",
    "        for rowNum, y in train[0].iterrows():\n",
    "            distances.append((getEuclideanDist(x, y), rowNum))\n",
    "        distances = sorted(distances, key=itemgetter(0))\n",
    "        for i in range(k):\n",
    "            if train[1].loc[[distances[i][1]]].values[0] in votes:\n",
    "                votes[train[1].loc[[distances[i][1]]].values[0]] += 1\n",
    "            else:\n",
    "                votes[train[1].loc[[distances[i][1]]].values[0]] = 1\n",
    "            if votes[train[1].loc[[distances[i][1]]].values[0]] > maxVotes:\n",
    "                maxVotes = votes[train[1].loc[[distances[i][1]]].values[0]]\n",
    "                maxClass = train[1].loc[[distances[i][1]]].values[0]\n",
    "        if test[1].iloc[[test_row_index]].values[0] == maxClass:\n",
    "            no_of_predictions += 1\n",
    "        test_row_index += 1\n",
    "    \n",
    "    #print(\"Accuracy Score is: --- \", no_of_predictions, test[0].shape[0], no_of_predictions/test[0].shape[0])\n",
    "    return float(no_of_predictions/test[0].shape[0])\n",
    "                \n",
    "        \n",
    "            \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FR5wy4ITdt0r"
   },
   "source": [
    "2.2 Use the data sets obtained in problem 1 to determine the optimal k over each data set for KNN algorithm. For 5 different k values, plot the test error for each data set. Total number of figures = 3 (data set number) Ã— 2 (distance function number) = 6. Report the best k and distance function for each data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "azjv696HbZec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score is: ---  91 138 0.6594202898550725\n",
      "Accuracy Score is: ---  84 138 0.6086956521739131\n",
      "Accuracy Score is: ---  77 138 0.5579710144927537\n",
      "Accuracy Score is: ---  82 138 0.5942028985507246\n",
      "Accuracy Score is: ---  92 138 0.6666666666666666\n",
      "Best k -->  1\n",
      "Max accuracy -->  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# Code for problem 2.2 goes here\n",
    "best_k = 0\n",
    "max_accuracy = -1\n",
    "for i in range(5):\n",
    "    for k in range(1, 8, 2):\n",
    "        accuracy = getKneighbors(trains_credit_encoded[i], tests_credit_encoded[i], k)\n",
    "        if accuracy > max_accuracy:\n",
    "            max_accuracy = accuracy\n",
    "            best_k = k\n",
    "\n",
    "print(\"Best k --> \", best_k)\n",
    "print(\"Max accuracy --> \", max_accuracy)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KxnADTO7dyVt"
   },
   "source": [
    "2.3 Use sklearn's KNN implementation for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pBetI-wad3Hw"
   },
   "outputs": [],
   "source": [
    "# Code for problem 2.3 goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fPUZRZj3b6Xr"
   },
   "source": [
    "## Problem 3: Naive Bayes Classifier [30 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EnyhChckcTVf"
   },
   "source": [
    "3.1 Implement Naive Bayes classifier. You may need to modify it for categorical variables. To handle unseen feature values, you may need to make use Laplace smoothing or the m-estimate of conditional probability method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LgcQ9m3dcX8h"
   },
   "outputs": [],
   "source": [
    "# Code for problem 3.1 goes here\n",
    "import numpy as np\n",
    "#Guassian Naive Bayes calculation for numerical data\n",
    "def guassianProb(colNum, featureVal, targetVal, train):\n",
    "    frames =[train[0], train[1]]\n",
    "    train1 = pd.concat(frames, axis=1)\n",
    "    mean = 0\n",
    "    var = 0\n",
    "    data = train1.groupby(train1.shape[1] - 1)\n",
    "    for i in range(len(data.mean()[colNum].index)):\n",
    "        if data.mean()[colNum].index[i] == targetVal:\n",
    "            mean = data.mean()[colNum].values[i]\n",
    "            var = data.var()[colNum].values[i]\n",
    "            break\n",
    "    if var == 0:\n",
    "        var = 0.001\n",
    "    prob = (1/math.sqrt(2*3.14*var)) * np.exp((-(featureVal-mean)**2)/(2*var))\n",
    "    #print(prob)\n",
    "    \n",
    "    return prob\n",
    "    \n",
    "    \n",
    "def naiveBayes(train, test):\n",
    "    likelihood = {}\n",
    "    prior = {}\n",
    "    frames =[train[0], train[1]]\n",
    "    train1 = pd.concat(frames, axis=1)\n",
    "    frames =[test[0], test[1]]\n",
    "    test1 = pd.concat(frames, axis=1)\n",
    "    \n",
    "    for col in train[0]:\n",
    "        if train[0][col].dtypes =='object':\n",
    "            data = train1.groupby([col,train1.shape[1] - 1]).size()\n",
    "            for i in range(len(data)):\n",
    "                if not col in likelihood:\n",
    "                    likelihood[col] = {}\n",
    "                likelihood[col][data.index[i][1]] = data[i]\n",
    "                for j in range(len(train1.groupby(train1.shape[1] - 1).size())):\n",
    "                    if data.index[i][1] == train1.groupby(train1.shape[1] - 1).size().index[j]:\n",
    "                        denom = train1.groupby(train1.shape[1] - 1).size().values[j]\n",
    "                        break\n",
    "                likelihood[col][data.index[i][1]] /= denom\n",
    "    #print(likelihood)\n",
    "    \n",
    "    for i in range(len(train1.groupby(train1.shape[1] - 1).size())):\n",
    "        prior[train1.groupby(train1.shape[1] - 1).size().index[i]] = train1.groupby(train1.shape[1] - 1).size().values[i] / sum(train1.groupby(train1.shape[1] - 1).size().values)\n",
    "\n",
    "    no_of_predictions = 0  \n",
    "    actual_target = \"\"\n",
    "    \n",
    "    for index,row in test1.iterrows():\n",
    "        actual_target = row[test1.shape[1] - 1]\n",
    "        max_posterior = -1\n",
    "        maxClass = \"\"\n",
    "        for target,val in prior.items():\n",
    "            posterior = 1\n",
    "            for i in range(len(row) - 1):\n",
    "                if test[0][i].dtypes == 'object':\n",
    "                    if row[i] in likelihood[i]:\n",
    "                        posterior *= likelihood[i][row[i]]\n",
    "                    else:\n",
    "                        posterior *= 1/(10**6)\n",
    "                    posterior *= prior[target]\n",
    "                else:\n",
    "                    posterior *= guassianProb(i, row[i], target, train)\n",
    "                    \n",
    "            if posterior > max_posterior:\n",
    "                max_posterior = posterior\n",
    "                maxClass = target\n",
    "        if actual_target == maxClass:\n",
    "            no_of_predictions += 1\n",
    "    #print(\"Accuracy score ---> \", no_of_predictions, test1.shape[0], no_of_predictions/test1.shape[0])\n",
    "    return float(no_of_predictions/test1.shape[0])\n",
    "            \n",
    "#naiveBayes(trains_cars[0], tests_cars[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H-Tt8ztecYUo"
   },
   "source": [
    "3.2 Train Naive Bayes classifiers over training data sets and test each classifier against corresponding test data. Make a plot that shows the error over each test data. Report the average error rate for 5-fold cross validation for each data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xk2-71GvcaAG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average 5-fold accuracy for credit -- => 0.6695652173913043\n",
      "Average 5-fold accuracy for Ionosphere -- => 0.7942857142857143\n",
      "Average 5-fold accuracy for Cars -- => 0.7008695652173913\n"
     ]
    }
   ],
   "source": [
    "# Code for problem 3.2 goes here\n",
    "acc_credit = 0\n",
    "acc_iono = 0\n",
    "acc_cars = 0\n",
    "for i in range(5):\n",
    "    acc_credit += naiveBayes(trains_credit[i], tests_credit[i])\n",
    "    acc_iono += naiveBayes(trains_ionosphere[i], tests_ionosphere[i])\n",
    "    acc_cars += naiveBayes(trains_cars[i], tests_cars[i])\n",
    "print(\"Average 5-fold accuracy for credit -- =>\", acc_credit/5)\n",
    "print(\"Average 5-fold accuracy for Ionosphere -- =>\", acc_iono/5)\n",
    "print(\"Average 5-fold accuracy for Cars -- =>\", acc_cars/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ehpK91vQcaUu"
   },
   "source": [
    "3.3 Use Naive Bayes package in sklearn for validation."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "P556-f18-a4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
